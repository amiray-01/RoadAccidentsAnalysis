{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des données d'accidents de la route\n",
    "\n",
    "## Récupération des données\n",
    "\n",
    "Tout d'abord, nous allons commencer par télécharger les données nécéssaire pour faire cette analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "#Liste des URLS\n",
    "urls = [\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/20231005-093927/carcteristiques-2022.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/20231005-094147/vehicules-2022.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/20231005-094229/usagers-2022.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2021/20231005-094112/lieux-2022.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2020/20221024-113743/carcteristiques-2021.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2020/20221024-113925/vehicules-2021.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2022/20231009-140337/usagers-2021.csv\",\n",
    "    \"https://static.data.gouv.fr/resources/bases-de-donnees-annuelles-des-accidents-corporels-de-la-circulation-routiere-annees-de-2005-a-2020/20221024-113901/lieux-2021.csv\"\n",
    "]\n",
    "\n",
    "data_directory = \"data\"\n",
    "os.makedirs(data_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on a besoin d'une fonction qui va nous permettre de télécharger ces fichiers la à partir de leurs URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, directory):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        print(f'Téléchargement de {filename} terminé, le fichier est sauvegardé dans {filepath}')\n",
    "    else:\n",
    "        print(f\"Impossible de télécharger le fichier {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus qu'a parcourir la liste des URLS et télécharger les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls:\n",
    "    download_file(url, data_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données\n",
    "\n",
    "Maintenant que nous avons téléchargé les données, nous allons les nettoyer pour les rendre plus facile à manipuler.\n",
    "\n",
    "On commence par la création d'une session spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DM1\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Définition d'un schéma pour chaque fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Définition des schémas pour les données\n",
    "from pyspark.sql.types import *;\n",
    "\n",
    "#schéma pour les caractéristiques_2022\n",
    "#on en a créer deux différents car il y a une différence de nommage entre les fichiers de 2021 et 2022 pour la colonne de l'id de l'accident (Accident_Id et Num_Acc)\n",
    "caracteristiques2022_schema = StructType([\n",
    "    StructField(\"Accident_Id\", LongType(), True),\n",
    "    StructField(\"jour\", IntegerType(), True),\n",
    "    StructField(\"mois\", IntegerType(), True),\n",
    "    StructField(\"an\", IntegerType(), True),\n",
    "    StructField(\"hrmn\", StringType(),True),\n",
    "    StructField(\"lum\", IntegerType(), True),\n",
    "    StructField(\"dep\", StringType(), True),\n",
    "    StructField(\"com\", StringType(), True),\n",
    "    StructField(\"agg\", IntegerType(), True),\n",
    "    StructField(\"int\", IntegerType(), True),\n",
    "    StructField(\"atm\", IntegerType(), True),\n",
    "    StructField(\"col\", IntegerType(), True),\n",
    "    StructField(\"adr\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "])\n",
    "\n",
    "#schéma pour les caractéristiques\n",
    "caracteristiques_schema = StructType([\n",
    "    StructField(\"Num_Acc\", LongType(), True),\n",
    "    StructField(\"jour\", IntegerType(), True),\n",
    "    StructField(\"mois\", IntegerType(), True),\n",
    "    StructField(\"an\", IntegerType(), True),\n",
    "    StructField(\"hrmn\", StringType(),True), \n",
    "    StructField(\"lum\", IntegerType(), True),\n",
    "    StructField(\"dep\", StringType(), True),\n",
    "    StructField(\"com\", StringType(), True),\n",
    "    StructField(\"agg\", IntegerType(), True),\n",
    "    StructField(\"int\", IntegerType(), True),\n",
    "    StructField(\"atm\", IntegerType(), True),\n",
    "    StructField(\"col\", IntegerType(), True),\n",
    "    StructField(\"adr\", StringType(), True),\n",
    "    StructField(\"lat\", StringType(), True),\n",
    "    StructField(\"long\", StringType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "#schéma pour les véhicules\n",
    "vehicules_schema = StructType([\n",
    "    StructField(\"Num_Acc\", LongType(), True),\n",
    "    StructField(\"id_vehicule\", StringType(), True),\n",
    "    StructField(\"num_veh\", StringType(), True),  \n",
    "    StructField(\"senc\", IntegerType(), True),\n",
    "    StructField(\"catv\", IntegerType(), True),\n",
    "    StructField(\"obs\", IntegerType(), True),\n",
    "    StructField(\"obsm\", IntegerType(), True),\n",
    "    StructField(\"choc\", IntegerType(), True),\n",
    "    StructField(\"manv\", IntegerType(), True),\n",
    "    StructField(\"motor\", IntegerType(), True),\n",
    "    StructField(\"occutc\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "#schéma pour les usagers\n",
    "usagers_schema = StructType([\n",
    "    StructField(\"Num_Acc\", LongType(), True),\n",
    "    StructField(\"id_usager\", StringType(), True),\n",
    "    StructField(\"id_vehicule\", StringType(), True),\n",
    "    StructField(\"num_veh\", StringType(), True),\n",
    "    StructField(\"place\", IntegerType(), True),\n",
    "    StructField(\"catu\", IntegerType(), True),\n",
    "    StructField(\"grav\", IntegerType(), True),\n",
    "    StructField(\"sexe\", IntegerType(), True),\n",
    "    StructField(\"An_nais\", IntegerType(), True),\n",
    "    StructField(\"trajet\", IntegerType(), True),\n",
    "    StructField(\"secu1\", IntegerType(), True),\n",
    "    StructField(\"secu2\", IntegerType(), True),\n",
    "    StructField(\"secu3\", IntegerType(), True),\n",
    "    StructField(\"locp\", IntegerType(), True),\n",
    "    StructField(\"actp\", IntegerType(), True),\n",
    "    StructField(\"etatp\", IntegerType(), True),\n",
    "])\n",
    "\n",
    "\n",
    "#schéma pour les lieux\n",
    "lieux_schema = StructType([\n",
    "    StructField(\"Num_Acc\", LongType(), True),\n",
    "    StructField(\"catr\", IntegerType(), True),\n",
    "    StructField(\"voie\", StringType(), True),\n",
    "    StructField(\"V1\", StringType(), True),\n",
    "    StructField(\"V2\", StringType(), True),\n",
    "    StructField(\"circ\", IntegerType(), True),\n",
    "    StructField(\"nbv\", IntegerType(), True),\n",
    "    StructField(\"vosp\", IntegerType(), True),\n",
    "    StructField(\"prof\", IntegerType(), True),\n",
    "    StructField(\"pr\", StringType(), True),\n",
    "    StructField(\"pr1\", StringType(), True),\n",
    "    StructField(\"plan\", IntegerType(), True),\n",
    "    StructField(\"lartpc\", FloatType(), True),\n",
    "    StructField(\"larrout\", FloatType(), True),\n",
    "    StructField(\"surf\", IntegerType(), True),\n",
    "    StructField(\"infra\", IntegerType(), True),\n",
    "    StructField(\"situ\", IntegerType(), True),\n",
    "    StructField(\"vma\", FloatType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chargement des fichiers et création des DataFrames à partir des fichiers téléchargés..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_to_df(spark, file_path, schema, delimiter=\";\"):\n",
    "    df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", delimiter) \\\n",
    "        .schema(schema) \\\n",
    "        .csv(file_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "file_path_caracterstiques_2022 = \"data/carcteristiques-2022.csv\"\n",
    "file_path_caracterstiques_2021 = \"data/carcteristiques-2021.csv\"\n",
    "file_path_vehicules_2022 = \"data/vehicules-2022.csv\"\n",
    "file_path_vehicules_2021 = \"data/vehicules-2021.csv\"\n",
    "file_path_usagers_2022 = \"data/usagers-2022.csv\"\n",
    "file_path_usagers_2021 = \"data/usagers-2021.csv\"\n",
    "file_path_lieux_2022 = \"data/lieux-2022.csv\"\n",
    "file_path_lieux_2021 = \"data/lieux-2021.csv\"\n",
    "\n",
    "\n",
    "df_caracteristiques_2022 = load_csv_to_df(spark, file_path_caracterstiques_2022, caracteristiques2022_schema)\n",
    "df_caracteristiques_2022 = df_caracteristiques_2022.withColumnRenamed(\"Accident_Id\", \"Num_Acc\")\n",
    "df_caracteristiques_2021 = load_csv_to_df(spark, file_path_caracterstiques_2021, caracteristiques_schema)\n",
    "df_vehicules_2022 = load_csv_to_df(spark, file_path_vehicules_2022, vehicules_schema)\n",
    "df_vehicules_2021 = load_csv_to_df(spark, file_path_vehicules_2021, vehicules_schema)\n",
    "df_usagers_2022 = load_csv_to_df(spark, file_path_usagers_2022, usagers_schema)\n",
    "df_usagers_2021 = load_csv_to_df(spark, file_path_usagers_2021, usagers_schema)\n",
    "df_lieux_2022 = load_csv_to_df(spark, file_path_lieux_2022, lieux_schema)\n",
    "df_lieux_2021 = load_csv_to_df(spark, file_path_lieux_2021, lieux_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on retrouve quelques fonctions générales qui vont nous faciliter la tâche lors du nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as fn\n",
    "\n",
    "#fonction qui permet de convertir les types de données\n",
    "def convert_data_types(df, type_mappings, date_format_mappings=None, timestamp_format_mappings=None):\n",
    "    for column, data_type in type_mappings.items():\n",
    "        df = df.withColumn(column, fn.col(column).cast(data_type))\n",
    "    \n",
    "    if date_format_mappings:\n",
    "        for column, fmt in date_format_mappings.items():\n",
    "            df = df.withColumn(column, fn.to_date(fn.col(column), fmt))\n",
    "    \n",
    "    if timestamp_format_mappings:\n",
    "        for column, fmt in timestamp_format_mappings.items():\n",
    "            df = df.withColumn(column, fn.to_timestamp(fn.col(column), fmt))\n",
    "    \n",
    "    return df\n",
    "\n",
    "#fonction qui permet de gérer les valeurs manquantes\n",
    "def handle_missing_values(df, fill_values=None, drop_rows=False, drop_cols=None):\n",
    "    if fill_values:\n",
    "        for column, fill_value in fill_values.items():\n",
    "            df = df.withColumn(column, fn.when(fn.col(column).isNull(), fill_value).otherwise(fn.col(column)))\n",
    "    \n",
    "    if drop_rows:\n",
    "        df = df.na.drop(subset=drop_rows)\n",
    "    \n",
    "    if drop_cols:\n",
    "        df = df.drop(*drop_cols)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import functools as ft\n",
    "#fonction qui permet d'indexer les colonnes catégorielles\n",
    "def index_categorical_columns(df, categorical_columns):\n",
    "    stages = []\n",
    "    \n",
    "    for categorical_col in categorical_columns:\n",
    "        string_indexer = StringIndexer(inputCol=categorical_col, outputCol=categorical_col + \"_indexed\")\n",
    "        stages.append(string_indexer)\n",
    "    \n",
    "    pipeline = Pipeline(stages=stages)\n",
    "    \n",
    "    df_transformed = pipeline.fit(df).transform(df)\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "#fonction qui affiche les lignes avec des valeurs nulles\n",
    "def show_lignes_nulles(df):\n",
    "    condition = ft.reduce(lambda a, b: a | b, (fn.col(c).isNull() for c in df.columns))\n",
    "    lignes_avec_nulles = df.filter(condition)\n",
    "    lignes_avec_nulles.show()\n",
    "\n",
    "#fonction qui affiche les lignes en doublons\n",
    "def nb_lignes_doublons(df):\n",
    "    duplicates = df.groupBy(df.columns) \\\n",
    "        .agg(fn.count(\"*\").alias(\"count\")) \\\n",
    "        .filter(\"`count` > 1\")\n",
    "\n",
    "\n",
    "    total_duplicates = duplicates.selectExpr(\"sum(count) - count(*) as total_duplicates\").collect()[0][\"total_duplicates\"]\n",
    "\n",
    "    print(f\"Nombre total de lignes en doublon : {total_duplicates}\")\n",
    "\n",
    "#fonction qui affiche pour chaque colonne, le nombre de lignes nulles\n",
    "def nb_colonnes_nulles(df):\n",
    "    df.select([fn.count(fn.when(fn.col(c).isNull(), c)).alias(c) for c in df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_missing_values(df,colonne):\n",
    "    total_count = df.count()\n",
    "    missing_count = df.filter(fn.col(colonne).isNull()).count()\n",
    "    non_missing_count = total_count - missing_count\n",
    "    missing_percentage = (missing_count/total_count) * 100\n",
    "\n",
    "    print(f\"Total de lignes : {total_count}\")\n",
    "    print(f\"Nombre de valeurs manquantes : {missing_count}\")\n",
    "    print(f\"Pourcenrage de valeurs manquantes : {missing_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "def frequence_values(df,colonne):\n",
    "    value_counts = df.filter(fn.col(colonne).isNotNull()) \\\n",
    "    .groupBy(colonne) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed(\"count\", \"frequency\") \\\n",
    "    .orderBy(fn.desc(\"frequency\"))\n",
    "\n",
    "# Ajouter une colonne avec la somme totale des fréquences sur toutes les lignes pour calculer le pourcentage\n",
    "    total_frequency = value_counts.select(fn.sum(\"frequency\").alias(\"total\")).collect()[0][\"total\"]\n",
    "    value_counts = value_counts.withColumn(\"percentage\", (fn.col(\"frequency\") / total_frequency) * 100)\n",
    "\n",
    "# Afficher le résultat\n",
    "    value_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La table `caracteristiques`\n",
    "\n",
    "Commençons par traiter la table `caracteristiques_2022` et en premier lieu le bon typage des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2022.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit rappel du schéma pour voir le type de chaque colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le schéma affiché, on peut facilement idenitifer les colonnes qui sont mal typées. Par exemple les colonnes `lat` et `long` sont de type `StringType` alors qu'elles devraient être de type `DoubleType`.\n",
    "\n",
    "\n",
    "On peut aussi ajouter une colonne `date_time` qui sera de type `TimeStamp` et qui sera la combinaison des colonnes `an`, `mois`, `jour` et `hrmn`.\n",
    "\n",
    "\n",
    "Pour le reste des colonnes, on peut les laisser telles quelles car elles sont bien typées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On en déduit donc cela (pour les colonnes lat et long et l'ajout de la colonne 'date_time', nous allons faire cela dans la fonction générale pour \n",
    "#le nettoyage de 'caracterstiques' pour bien respecter le principe DRY)\n",
    "timestamp_format_mappings = {\"date_temp\": \"yyyy-MM-dd HH:mm\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons au traitement des valeurs manquantes dans la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_caracteristiques_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_caracteristiques_2022,\"col\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous constatons ici la présence de quelques valeurs manquantes (minimes), nous allons les traiter en les remplaçant par des valeurs par défaut.\n",
    "\n",
    "Pour l'analyse des données nous aurons juste a filter les lignes qui contiennent des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {'lum':-1,'int':-1,'atm':-1,'col':-1,'adr' : \"adresse non renseigné\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il nous reste maintenant à identifier les colonnes catégorielles.\n",
    "\n",
    "Et à partir des informations fourni pour la table `caracteristiques`, on peut identifier les colonnes catégorielles suivant: \n",
    "`lum`, `agg`, `int`, `atm` et `col`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_caracteristiques = [\"lum\",\"agg\", \"int\", \"atm\", \"col\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant construire notre fonction `clean_caracteristiques` qui va nous permettre de nettoyer la table `caracteristiques` en appliquant les traitements que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fonction qui permet de nettoyer les données des caractéristiques des accidents\n",
    "def clean_caracteristiques(df,fill_values=None,drop_rows=False,drop_cols=False,type_mapping=None,date_format_mappings=None,timestamp_format_mappings=None,categorical_columns=None):\n",
    "    df = handle_missing_values(df, fill_values=fill_values, drop_rows=drop_rows, drop_cols=drop_cols)\n",
    "    df = df.withColumn(\n",
    "    \"date_temp\",\n",
    "    fn.concat_ws(\" \", \n",
    "              fn.concat_ws(\"-\", fn.col(\"an\"), fn.lpad(fn.col(\"mois\"), 2, \"0\"), fn.lpad(fn.col(\"jour\"), 2, \"0\")),\n",
    "              fn.col(\"hrmn\"))\n",
    "    )\n",
    "    df = convert_data_types(df, type_mappings=type_mapping, date_format_mappings=date_format_mappings, timestamp_format_mappings=timestamp_format_mappings)\n",
    "\n",
    "    #ici nous avons remarqué que lors du cast direct de ces deux colonnes en double, il y a eu des erreurs de conversion(des nulls partout)\n",
    "    #nous avons ensuite remarqué qu'il y avait des virgules à la place des points pour les valeurs décimales\n",
    "    \n",
    "    df = df \\\n",
    "        .withColumn(\"lat\", fn.regexp_replace(\"lat\", \",\", \".\")) \\\n",
    "        .withColumn(\"long\", fn.regexp_replace(\"long\", \",\", \".\"))\n",
    "\n",
    "    df = df.withColumn(\"lat\", fn.col(\"lat\").cast(\"double\")) \\\n",
    "        .withColumn(\"long\", fn.col(\"long\").cast(\"double\"))\n",
    "\n",
    "    df = df.withColumnRenamed(\"date_temp\", \"date_time\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuons maintenant le nettoyage de la table `caracteristiques`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "df_caracteristiques_2022 = clean_caracteristiques(\n",
    "    df_caracteristiques_2022, \n",
    "    fill_values=fill_values, \n",
    "    drop_rows={},\n",
    "    drop_cols={},\n",
    "    type_mapping={}, \n",
    "    timestamp_format_mappings=timestamp_format_mappings, \n",
    "    categorical_columns=categorical_columns_caracteristiques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques vérifications pour s'assurer que le nettoyage a bien été effectué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2022.show(5)\n",
    "nb_colonnes_nulles(df_caracteristiques_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant effectuer exactement le meme traitement pour la table `caracteristiques_2021`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques vérifications initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2021.show(5)\n",
    "nb_colonnes_nulles(df_caracteristiques_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_caracteristiques_2021,\"col\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similaire a celle de 2022, nous allons donc effectuer les memes opérations pour cette table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {\"atm\": -1, \"col\": -1}\n",
    "df_caracteristiques_2021 = clean_caracteristiques(\n",
    "    df_caracteristiques_2021, \n",
    "    fill_values=fill_values, \n",
    "    drop_rows={}, \n",
    "    drop_cols={},\n",
    "    type_mapping={}, \n",
    "    timestamp_format_mappings=timestamp_format_mappings, \n",
    "    categorical_columns=categorical_columns_caracteristiques)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous procédons encore une fois aux vérifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2021.show(5)\n",
    "nb_colonnes_nulles(df_caracteristiques_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_2021.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons juste comparer les deux schémas pour voir si les deux tables ont le meme pour pouvoir les fusionner par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_caracteristiques_2022.schema == df_caracteristiques_2021.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La table `vehicules`\n",
    "Commençons par la table `vehicules_2022` et en premier lieu le bon typage des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2022.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous pouvons constater que la colonne `id_vehicule` est mal typé, qui devrait être de type `integer`, pour le reste des colonnes, elles sont bien typées.\n",
    "\n",
    "Mais pour cela, nous devons d'abord supprimer les espaces entre les chiffres, car si nous effectuons la conversion directement\n",
    "nous aurons des nulles dans toutes les valeurs `id_vehicule`\n",
    "\n",
    "Cette opération sera effectuée directement dans la fonction `clean_vehicules`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\"id_vehicule\": IntegerType()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons donc au traitement des valeurs manquantes dans la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_vehicules_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons le pourcentage de valeurs manquantes dans la table `vehicules_2022` pour voir comment les traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_vehicules_2022,\"senc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les colonnes `senc`, `catv`, `obs`, `obsm`, `choc`, `manv` et `motor`, nous allons les remplacer par des valeurs par défaut car le pourcentage de valeurs manquantes est minime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {\"senc\": -1, \"catv\": -1, \"obs\": -1, \"obsm\": -1, \"choc\": -1, \"manv\": -1, \"motor\": -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour `occutc` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_vehicules_2022,\"occutc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prés de la totalité des lignes n'ont pas de valeurs pour cette colonne, nous allons donc la supprimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"occutc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à l'identification des colonnes catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir des informations fournies pour la table `vehicules`, on peut identifier les colonnes catégorielles suivantes:\n",
    "`senc`, `catv`, `obs`, `obsm`, `choc`, `manv`, `motor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_vehicules = [\"senc\", \"catv\", \"obs\", \"obsm\", \"choc\", \"manv\", \"motor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant construire notre fonction `clean_vehicules` qui va nous permettre de nettoyer les tables `vehicules` en appliquant les traitements que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vehicules(df, fill_values=None, drop_rows=False, drop_cols=False, type_mapping=None, date_format_mappings=None, timestamp_format_mappings=None, categorical_columns=None):\n",
    "    df = df.withColumn(\"id_vehicule\", fn.regexp_replace(fn.col(\"id_vehicule\"), \"[^\\\\d]\", \"\"))\n",
    "    df = handle_missing_values(df, fill_values=fill_values, drop_rows=drop_rows, drop_cols=drop_cols)\n",
    "    df = convert_data_types(df, type_mappings=type_mapping, date_format_mappings=date_format_mappings, timestamp_format_mappings=timestamp_format_mappings)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2022 = clean_vehicules(\n",
    "    df_vehicules_2022, \n",
    "    fill_values=fill_values, \n",
    "    drop_rows={}, \n",
    "    drop_cols=drop_cols,\n",
    "    type_mapping=type_mapping, \n",
    "    date_format_mappings={}, \n",
    "    timestamp_format_mappings={}, \n",
    "    categorical_columns=categorical_columns_vehicules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifications du nettoyage de la table `vehicules_2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2022.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2022.show(5)\n",
    "nb_colonnes_nulles(df_vehicules_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons effectuer encore une fois le meme traitement pour la table `vehicules_2021`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2021.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2021.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne mal typé est la meme que celle de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_vehicules_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois similaire a celle de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_vehicules_2021,\"senc\")\n",
    "info_missing_values(df_vehicules_2021,\"occutc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc garder les memes arguments et appeler la fonction `clean_vehicules`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2021 = clean_vehicules(\n",
    "    df_vehicules_2021, \n",
    "    fill_values=fill_values, \n",
    "    drop_rows={}, \n",
    "    drop_cols=drop_cols,\n",
    "    type_mapping=type_mapping, \n",
    "    date_format_mappings={}, \n",
    "    timestamp_format_mappings={}, \n",
    "    categorical_columns=categorical_columns_vehicules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérifications du nettoyage de la table `vehicules_2021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2021.show(5)\n",
    "nb_colonnes_nulles(df_vehicules_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_2021.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_vehicules_2022.schema == df_vehicules_2021.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La table `usagers`\n",
    "On commence par regarder le bon typage des colonnes de la table `usagers_2022`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2022.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit rappel du schéma pour voir le type de chaque colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous pouvons constater que la colonne `id_usager` et `id_vehicule` sont mal typé, qui devrait être de type `long` pour la première et de type `integer` pour la deuxième. Pour le reste des colonnes, elles sont bien typées.\n",
    "\n",
    "Mais pour cela, nous devons d'abord supprimer les espaces entre les chiffres, car si nous effectuons la conversion directement\n",
    "nous aurons des nulles dans toutes les valeurs `id_usager` et `id_vehicule`\n",
    "\n",
    "Cette opération sera effectuée directement dans la fonction `clean_usagers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    'id_vehicule': IntegerType(),\n",
    "    'id_usager': LongType(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons donc au traitement des valeurs manquantes dans la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_usagers_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_lignes_nulles(df_usagers_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons le pourcentage de valeurs manquantes pour la colonne `sexe` dans la table `df_usagers_2022` pour voir comment les traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_usagers_2022,\"sexe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant la frequence des valeurs pour la colonne `sexe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_values(df_usagers_2022, \"sexe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la valeur `1` est la plus fréquente, nous allons donc la remplacer par cette valeur. Ça sera la meme chose our les colonnes `grav`, `trajet`, `secu1` et `place` car le pourcentage de valeurs manquantes est minime et n'affectera pas l'analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la colonne `An_nais`, nous avons pris la convention de remplacer les valeurs manquantes par `9999` pour signifier que l'année de naissance est inconnue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le reste des colonnes ayant des valeurs manquantes, nous les avons pas remplacer par la valeur la plus fréquente mais plutot par la valeur `-1` pour signifier que la valeur est non rensignée, car le pourcentage de valeurs manquantes est assez élevé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {\n",
    "    'sexe': df_usagers_2022.groupBy(\"sexe\").count().orderBy(\"count\", ascending=False).first()[0],  # Remplacer les valeurs nulles dans 'sexe' par 1\n",
    "    'An_nais': 9999,  # Remplacer les valeurs nulles dans 'An_nais' par 9999\n",
    "    'grav' : df_usagers_2022.groupBy(\"grav\").count().orderBy(\"count\", ascending=False).first()[0],\n",
    "    'trajet' : df_usagers_2022.groupBy(\"trajet\").count().orderBy(\"count\", ascending=False).first()[0],\n",
    "    'secu1' : df_usagers_2022.groupBy(\"secu1\").count().orderBy(\"count\", ascending=False).first()[0],\n",
    "    'place' : df_usagers_2022.groupBy(\"place\").count().orderBy(\"count\", ascending=False).first()[0],\n",
    "    'secu2' : -1,\n",
    "    'locp' : -1,\n",
    "    'actp' : -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour `secu3` ainsi que `etatp` ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_usagers_2022, \"secu3\")\n",
    "info_missing_values(df_usagers_2022, \"etatp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prés de la totalité des lignes n'ont pas de valeurs pour ces 2 colonnes, nous allons donc les supprimer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['secu3', 'etatp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à l'identification des colonnes catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir des informations fournies pour la table `usager-2022`, on peut identifier les colonnes catégorielles suivantes:\n",
    "`catu`, `grav`, `sexe`, `trajet`, `secu1`, `secu2`, `locp` et `actp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_usagers = ['catu', 'grav', 'sexe', 'trajet', 'secu1', 'secu2', 'locp', 'actp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant construire notre fonction `clean_usagers` qui va nous permettre de nettoyer les tables `usagers` en appliquant les traitements que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_usagers(df,fill_values=None,drop_rows=False,drop_cols=None,categorical_columns=None, type_mapping=None):\n",
    "    df = df.withColumn(\"id_vehicule\", fn.regexp_replace(fn.col(\"id_vehicule\"), \"[^\\\\d]\", \"\"))\n",
    "    df = df.withColumn(\"id_usager\", fn.regexp_replace(fn.col(\"id_usager\"), \"[^\\\\d]\", \"\"))\n",
    "    df = handle_missing_values(df, fill_values=fill_values, drop_rows=drop_rows, drop_cols=drop_cols)\n",
    "    df = convert_data_types(df, type_mappings=type_mapping, date_format_mappings=None, timestamp_format_mappings=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = False\n",
    "\n",
    "df_usagers_2022 = clean_usagers(df_usagers_2022,\n",
    "                                fill_values,\n",
    "                                drop_rows, \n",
    "                                columns_to_drop, \n",
    "                                categorical_columns_usagers,\n",
    "                                type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification du nettoyage de la table `usagers_2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2022.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2022.show(5)\n",
    "nb_colonnes_nulles(df_usagers_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons effectuer encore une fois le meme traitement pour la table `usagers_2021`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2021.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2021.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes mal typées sont les memes que celles de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_usagers_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois similaire à celle de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_usagers_2021, \"secu3\")\n",
    "info_missing_values(df_usagers_2021, \"sexe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_values(df_usagers_2021, \"sexe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc garder les memes arguments et appeler la fonction `clean_usagers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = False\n",
    "df_usagers_2021 = clean_usagers(df_usagers_2021,\n",
    "                                fill_values, \n",
    "                                drop_rows,\n",
    "                                columns_to_drop, \n",
    "                                categorical_columns_usagers, \n",
    "                                type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vériifcation du nettoyage de la table `usagers_2021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2021.show(5)\n",
    "nb_colonnes_nulles(df_usagers_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers_2021.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_usagers_2022.schema == df_usagers_2021.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La table `Lieux`\n",
    "On commence par regarder le bon typage des colonnes de la table `lieux_2022`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2022.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_values(df_lieux_2022, \"pr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on remarque que les colonnes `pr` et `pr1` ont des valeurs entières et des valeurs `(1)` qui font surement reference à des valeurs non renseignées. ( La description de la table indique que `-1` signifie `non renseigné`, et on l'absence de cette valeur, on peut supposer que `(1)` est utilisé pour signifier `non renseigné`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc remplacer ces valeurs par `-1` pour les colonnes `pr` et `pr1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_modify = ['pr', 'pr1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit rappel du schéma pour voir le type de chaque colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2022.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, nous pouvons constater que la colonne `pr` et `pr1` sont mal typé, qui devrait être tout les 2 de type `integer`. Pour le reste des colonnes, elles sont bien typées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\n",
    "    'pr' : IntegerType(),\n",
    "    'pr1' : IntegerType(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons donc au traitement des valeurs manquantes dans la table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_lieux_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_lignes_nulles(df_lieux_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons le pourcentage de valeurs manquantes pour la colonne `vosp` dans la table `df_lieux_2022` pour voir comment les traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_lieux_2022, \"vosp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant la frequence des valeurs pour la colonne `vosp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_values(df_lieux_2022, \"vosp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la valeur `0` est la plus fréquente, nous allons donc la remplacer par cette valeur. Ça sera la meme chose our les colonnes `circ`, `nbv`, `prof`, `plan`, `larrout`, `surf`, `infra` `situ` et `vma` car le pourcentage de valeurs manquantes est minime et n'affectera pas l'analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_values = {\n",
    "    'circ': df_lieux_2022.groupBy(\"circ\").count().orderBy(\"circ\", ascending=False).first()[0],\n",
    "    'nbv': df_lieux_2022.groupBy(\"nbv\").count().orderBy(\"nbv\", ascending=False).first()[0],\n",
    "    'vosp' : df_lieux_2022.groupBy(\"vosp\").count().orderBy(\"vosp\", ascending=False).first()[0],\n",
    "    'prof' : df_lieux_2022.groupBy(\"prof\").count().orderBy(\"prof\", ascending=False).first()[0],\n",
    "    'plan' : df_lieux_2022.groupBy(\"plan\").count().orderBy(\"plan\", ascending=False).first()[0],\n",
    "    'larrout' : df_lieux_2022.groupBy(\"larrout\").count().orderBy(\"larrout\", ascending=False).first()[0],\n",
    "    'surf' : df_lieux_2022.groupBy(\"surf\").count().orderBy(\"surf\", ascending=False).first()[0],\n",
    "    'infra' : df_lieux_2022.groupBy(\"infra\").count().orderBy(\"infra\", ascending=False).first()[0],\n",
    "    'situ' : df_lieux_2022.groupBy(\"situ\").count().orderBy(\"situ\", ascending=False).first()[0],\n",
    "    'vma' : df_lieux_2022.groupBy(\"vma\").count().orderBy(\"vma\", ascending=False).first()[0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_lieux_2022, \"lartpc\")\n",
    "frequence_values(df_lieux_2022, \"V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Près de la totalité des lignes de la colonne `lartpc` n'ont pas de valeurs, et pour ce qui est de la colonne `V2`, plus de `90%` des valeurs sont non renseignées. Nous allons donc supprimer ces 2 colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['lartpc','V2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passons à l'identification des colonnes catégorielles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "À partir des informations fournies pour la table `lieux`, on peut identifier les colonnes catégorielles suivantes:\n",
    "`catr`, `circ`, `vosp`, `prof`, `plan`, `surf`, `infra` et `situ`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns_lieux = ['catr', 'circ', 'vosp', 'prof', 'plan', 'surf', 'infra', 'situ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant construire notre fonction `clean_lieux` qui va nous permettre de nettoyer les tables `lieux` en appliquant les traitements que nous avons défini plus haut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lieux(df,fill_values=None,drop_rows=False,drop_cols=None,categorical_columns=None, type_mapping=None, columns_to_modify=None):\n",
    "    for column in columns_to_modify:\n",
    "        df = df.withColumn(column, fn.when(fn.col(column) == '(1)', '-1').otherwise(fn.col(column)))\n",
    "        \n",
    "    df = handle_missing_values(df, fill_values=fill_values, drop_rows=drop_rows, drop_cols=drop_cols)\n",
    "    df = convert_data_types(df, type_mappings=type_mapping, date_format_mappings=None, timestamp_format_mappings=None)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = False\n",
    "\n",
    "df_lieux_2022 = clean_lieux(df_lieux_2022,\n",
    "                            fill_values, \n",
    "                            drop_rows, \n",
    "                            columns_to_drop, \n",
    "                            categorical_columns_lieux, \n",
    "                            type_mapping, \n",
    "                            columns_to_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vériifcation du nettoyage de la table `lieux_2022`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2022.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2022.show(5)\n",
    "nb_colonnes_nulles(df_lieux_2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons effectuer encore une fois le meme traitement pour la table `lieux_2021`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2021.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2021.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les colonnes mal typées sont les memes que celles de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_colonnes_nulles(df_lieux_2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois similaire à celle de 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_missing_values(df_lieux_2021, \"lartpc\")\n",
    "info_missing_values(df_lieux_2021, \"vosp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequence_values(df_lieux_2021, \"V2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons donc garder les memes arguments et appeler la fonction `clean_lieux`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_rows = False\n",
    "\n",
    "df_lieux_2021 = clean_lieux(df_lieux_2021, \n",
    "                            fill_values, \n",
    "                            drop_rows, \n",
    "                            columns_to_drop, \n",
    "                            categorical_columns_lieux, \n",
    "                            type_mapping, \n",
    "                            columns_to_modify)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vériifcation du nettoyage de la table `lieux_2021`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2021.show(5)\n",
    "nb_colonnes_nulles(df_lieux_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux_2021.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_lieux_2022.schema == df_lieux_2021.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réunion des tables 2021 et 2022 et analyse des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réunion des données\n",
    "\n",
    "Maintenant que le nettoyages des données est terminé, nous allons fusionner les tables 'caracteristiques','vehciules','usagers' et 'lieux'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques = df_caracteristiques_2021.unionByName(df_caracteristiques_2022)\n",
    "\n",
    "df_vehicules_2021 = df_vehicules_2021.withColumn(\"annee\", fn.lit(2021))\n",
    "df_vehicules_2022 = df_vehicules_2022.withColumn(\"annee\",fn.lit(2022))\n",
    "df_vehicules = df_vehicules_2021.unionByName(df_vehicules_2022)\n",
    "\n",
    "df_usagers_2021 = df_usagers_2021.withColumn(\"annee\", fn.lit(2021))\n",
    "df_usagers_2022 = df_usagers_2022.withColumn(\"annee\", fn.lit(2022))\n",
    "df_usagers = df_usagers_2021.unionByName(df_usagers_2022)\n",
    "\n",
    "df_lieux_2021 = df_lieux_2021.withColumn(\"annee\", fn.lit(2021))\n",
    "df_lieux_2022 = df_lieux_2022.withColumn(\"annee\", fn.lit(2022))\n",
    "df_lieux = df_lieux_2021.unionByName(df_lieux_2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre total de lignes : {df_caracteristiques.count()}\")\n",
    "print(f\"Nombre total de lignes : {df_vehicules.count()}\")\n",
    "print(f\"Nombre total de lignes : {df_usagers.count()}\")\n",
    "print(f\"Nombre total de lignes : {df_lieux.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_caracteristiques.count() == df_caracteristiques_2022.count()+df_caracteristiques_2021.count())\n",
    "print(df_vehicules.count() == df_vehicules_2022.count()+df_vehicules_2021.count())\n",
    "print(df_usagers.count() == df_usagers_2022.count()+df_usagers_2021.count())\n",
    "print(df_lieux.count() == df_lieux_2022.count()+df_lieux_2021.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque les données sont maintenant fusionnés, nous allons pouvoir traiter les colonnes catégorielles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques = index_categorical_columns(df_caracteristiques, categorical_columns_caracteristiques)\n",
    "df_vehicules = index_categorical_columns(df_vehicules, categorical_columns_vehicules)\n",
    "df_usagers = index_categorical_columns(df_usagers, categorical_columns_usagers)\n",
    "df_lieux = index_categorical_columns(df_lieux, categorical_columns_lieux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques.show(5)\n",
    "df_vehicules.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers.show(5)\n",
    "df_lieux.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résumés numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons ici la colonne \"lum\" qui represente les conditions d’éclairage dans lesquelles l'accident s'est produit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrons notre dataframe pour se débarassé des '-1' qui sont des valeurs non renseigné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_caracterstiques = df_caracteristiques.filter(fn.col(\"lum\") != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_caracterstiques.select(\n",
    "    fn.mean('lum').alias('Moyenne'),\n",
    "    fn.stddev('lum').alias('Ecart-type'),\n",
    "    fn.skewness('lum').alias('Asymétrie'),\n",
    "    fn.kurtosis('lum').alias('Aplatissement'),\n",
    "    fn.expr('percentile(lum, array(0.25))')[0].alias('1er Quartile'),\n",
    "    fn.expr('percentile(lum, array(0.5))')[0].alias('Médiane'),\n",
    "    fn.expr('percentile(lum, array(0.75))')[0].alias('3ème Quartile')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, afin d'engendrer les objets graphiques, nous allons transformer notre dataframe spark en une dataframe pandas pour l'utilisation des fonctions de visualisation de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_caracteristiques = df_caracteristiques.toPandas()\n",
    "filtered_df_caracterstiques_lum = pandas_df_caracteristiques[pandas_df_caracteristiques[\"lum\"] != -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelques objets graphiques pour illustrer les données de la colonne \"lum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.box(filtered_df_caracterstiques_lum, y='lum', title='Distribution des conditions d\\'éclairage')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.data_transformers.disable_max_rows()\n",
    "\n",
    "chart = alt.Chart(filtered_df_caracterstiques_lum).mark_boxplot().encode(\n",
    "    y='lum:Q',\n",
    ").properties(\n",
    "    title='Distribution des conditions d\\'éclairage avec Altair'\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons effectuer les memes étapes avec differentes colonnes \"catv\" pour l'étude des catégories de véhicules ici en l'occurence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_vehicules = df_vehicules.filter(fn.col(\"catv\") != -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_vehicules.select(\n",
    "    fn.mean('catv').alias('Moyenne'),\n",
    "    fn.stddev('catv').alias('Ecart-type'),\n",
    "    fn.skewness('catv').alias('Asymétrie'),\n",
    "    fn.kurtosis('catv').alias('Aplatissement'),\n",
    "    fn.expr('percentile(catv, array(0.25))')[0].alias('1er Quartile'),\n",
    "    fn.expr('percentile(catv, array(0.5))')[0].alias('Médiane'),\n",
    "    fn.expr('percentile(catv, array(0.75))')[0].alias('3ème Quartile')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_vehicules = df_vehicules.toPandas()\n",
    "filtered_df_vehicules_catv = pandas_df_vehicules[pandas_df_vehicules[\"catv\"] != -1]\n",
    "fig = px.box(filtered_df_vehicules_catv, y='catv', title='Boxplot des Catégories de Véhicules')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(filtered_df_vehicules_catv).mark_boxplot().encode(\n",
    "    y='catv:Q',\n",
    ").properties(\n",
    "    title='Catégories de véhicules impliqués dans les accidents'\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meme chose ici pour la colonne \"An_nais\" qui represente l'année de naissance des usagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_usagers = df_usagers.filter(fn.col(\"An_nais\") != 9999)\n",
    "filtered_df_usagers.select(\n",
    "    fn.mean('An_nais').alias('Moyenne'),\n",
    "    fn.stddev('An_nais').alias('Ecart-type'),\n",
    "    fn.skewness('An_nais').alias('Asymétrie'),\n",
    "    fn.kurtosis('An_nais').alias('Aplatissement'),\n",
    "    fn.expr('percentile(An_nais, array(0.25))')[0].alias('1er Quartile'),\n",
    "    fn.expr('percentile(An_nais, array(0.5))')[0].alias('Médiane'),\n",
    "    fn.expr('percentile(An_nais, array(0.75))')[0].alias('3ème Quartile')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_usagers = df_usagers.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sauf qu'ici, au lieu de traiter directement la colonen 'An_nais', nous allons plutôt créer une colonne 'age' qui representra l'age des usagers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_usagers_age = pandas_df_usagers[pandas_df_usagers['An_nais'] != 9999].copy()\n",
    "filtered_df_usagers_age['age'] = filtered_df_usagers_age['annee'] - filtered_df_usagers_age['An_nais']\n",
    "\n",
    "#affiche quelques lignes de cette dataframe\n",
    " \n",
    "print(f\"La moyenne d'age est de : {filtered_df_usagers_age['age'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(filtered_df_usagers_age).mark_boxplot().encode(\n",
    "    y='age:Q',\n",
    ").properties(\n",
    "    title='Distribution de l\\'age des usagers impliqués dans les accidents'\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation pour la répartition des accidents sur la semaine(jours et heurs) ainsi que les mois de l'année"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons construire une dataframe qui contiendra en détails les colonnes nécéssaire à ce traitement (comme jour de la semaine)\n",
    "(les jours de la semaine sont numérotés de 1 à 7 commençant par le dimanche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques_rep = df_caracteristiques.withColumn(\"jour_semaine\", fn.dayofweek(\"date_time\"))\n",
    "df_caracteristiques_rep = df_caracteristiques_rep.withColumn(\"heure\", fn.hour(\"date_time\"))\n",
    "\n",
    "df_caracteristiques_rep = df_caracteristiques_rep.withColumn(\"jour_heure\", \n",
    "                                                              fn.concat(fn.col(\"jour_semaine\"), fn.lit(\"-\"), fn.col(\"heure\")))\n",
    "df_caracteristiques_rep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant visualiser cela en utilisant une heatmap pour avoir plus de détails (jour et heure réuni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion de Spark DataFrame en Pandas DataFrame pour la visualisation\n",
    "pandas_df = df_caracteristiques_rep.toPandas()\n",
    "\n",
    "heatmap_data = pandas_df.groupby('jour_heure').size().reset_index(name='counts')\n",
    "\n",
    "# Extraire le jour de la semaine et l'heure dans des colonnes séparées pour le graphique\n",
    "heatmap_data['jour'] = heatmap_data['jour_heure'].apply(lambda x: x.split(\"-\")[0])\n",
    "heatmap_data['heure'] = heatmap_data['jour_heure'].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "# Convertir 'heure' en chaîne pour garantir l'ordre lors de la visualisation\n",
    "heatmap_data['heure'] = heatmap_data['heure'].astype(str)\n",
    "\n",
    "# Définir l'ordre des heures de manière explicite\n",
    "hours_order = [str(i) for i in range(24)]\n",
    "\n",
    "# Création du heatmap avec Plotly\n",
    "fig_heatmap = px.density_heatmap(heatmap_data, x='heure', y='jour', z='counts', color_continuous_scale='Viridis', \n",
    "                                 title=\"Heatmap de la répartition des accidents par jour de la semaine et heure\",\n",
    "                                 category_orders={'heure': hours_order}) # Ajout de category_orders\n",
    "fig_heatmap.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons les histogrames pour les jours uniquement, les heures uniquement et les mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Répartition des accidents sur la semaine\n",
    "fig_semaine = px.histogram(pandas_df, x=\"jour_semaine\", nbins=7, title=\"Répartition des accidents par jour de la semaine\")\n",
    "fig_semaine.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des accidents sur les heures de la journée\n",
    "fig_heures = px.histogram(pandas_df, x=\"heure\", nbins=24, title=\"Répartition des accidents par heure de la journée\")\n",
    "fig_heures.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Répartition des accidents sur les mois de l'année\n",
    "fig_mois = px.histogram(pandas_df, x=\"mois\", nbins=12, title=\"Répartition des accidents par mois\")\n",
    "fig_mois.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Profils des usagers\n",
    "\n",
    "afin de générer le graphique pour le profil des usagers, nous allons effectuer une jointure entre la table 'usagers' et la table 'caracteristiques' pour avoir les informations sur les accidents et les usagers avec lesquels ils sont liés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined_cara_usagers = df_caracteristiques.join(df_usagers, \"Num_Acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on récupere les colonnes qui nous intéressent pour cette analyse ici en l'occurence \"agg\" pour le type d'agglomération et \"catu\" pour la catégorie d'usager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_profile = df_joined_cara_usagers.groupBy(\"agg\", \"catu\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df_profile = df_profile.toPandas()\n",
    "\n",
    "# Convertir 'agg'  et catu en texte pour améliorer la lisibilité dans le graphique\n",
    "pandas_df_profile['agg'] = pandas_df_profile['agg'].map({1: 'Hors agglomération', 2: 'En agglomération'})\n",
    "pandas_df_profile['catu'] = pandas_df_profile['catu'].map({1: 'Conducteur', 2: 'Passager', 3: 'Piéton'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(pandas_df_profile, x='catu', y='count', color='agg', barmode='group',\n",
    "             labels={'catu':'Catégorie d\\'usager', 'count':'Nombre d\\'usagers'},\n",
    "             title='Profil des usagers impliqués par lieu d\\'accident')\n",
    "fig.update_xaxes(type='category')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accidents impliquant des cyclistes et/ou des piétons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrons d'abord les cyclistes et les piétons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accidents impliquant des cyclistes\n",
    "df_cyclistes = df_vehicules.filter(df_vehicules.catv == 1)\n",
    "\n",
    "# Accidents impliquant des piétons\n",
    "df_pietons = df_usagers.filter(df_usagers.catu == 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite nous allons récuperer les accidents impliquant des cyclistes et des piétons sans doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_cyclistes = df_cyclistes.select(\"Num_Acc\").distinct()\n",
    "accidents_pietons = df_pietons.select(\"Num_Acc\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fusionne les accidents puis on fait une jointure avec la dataframe 'caracteristiques_rep' pour obtenur les détails des accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accidents_impliquant_pietons_ou_cyclistes = accidents_cyclistes.union(accidents_pietons).distinct()\n",
    "df_accidents_impliquant_pietons_ou_cyclistes = df_caracteristiques_rep.join(accidents_impliquant_pietons_ou_cyclistes, \"Num_Acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_accidents_impliquant_pietons_ou_cyclistes.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df_accidents_impliquant_pietons_ou_cyclistes.toPandas()\n",
    "\n",
    "heatmap_data = pandas_df.groupby('jour_heure').size().reset_index(name='counts')\n",
    "\n",
    "# Extraire le jour de la semaine et l'heure dans des colonnes séparées pour le graphique\n",
    "heatmap_data['jour'] = heatmap_data['jour_heure'].apply(lambda x: x.split(\"-\")[0])\n",
    "heatmap_data['heure'] = heatmap_data['jour_heure'].apply(lambda x: x.split(\"-\")[1])\n",
    "\n",
    "heatmap_data['heure'] = heatmap_data['heure'].astype(str)\n",
    "\n",
    "# Définir l'ordre des heures de manière explicite\n",
    "hours_order = [str(i) for i in range(24)]\n",
    "\n",
    "# Création du heatmap avec Plotly\n",
    "fig_heatmap = px.density_heatmap(heatmap_data, x='heure', y='jour', z='counts', color_continuous_scale='Viridis', \n",
    "                                 title=\"Heatmap de la répartition des accidents par jour de la semaine et heure\",\n",
    "                                 category_orders={'heure': hours_order}) # Ajout de category_orders\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pareil que tout à l'heure, on affiche les histogrammes pour les jours, les heures et les mois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jour = df_accidents_impliquant_pietons_ou_cyclistes.groupBy(\"jour_semaine\").count()\n",
    "pandas_df_jour = df_jour.toPandas()\n",
    "\n",
    "fig_jour = px.bar(pandas_df_jour, x='jour_semaine', y='count', title='Répartition des accidents impliquant des cyclistes/piétons par jour de la semaine')\n",
    "fig_jour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_heure = df_accidents_impliquant_pietons_ou_cyclistes.groupBy(\"heure\").count()\n",
    "pandas_df_heure = df_heure.toPandas()\n",
    "\n",
    "fig_heure = px.bar(pandas_df_heure, x='heure', y='count', title='Répartition des accidents impliquant des cyclistes/piétons par heure de la journée')\n",
    "fig_heure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mois = df_accidents_impliquant_pietons_ou_cyclistes.groupBy(\"mois\").count()\n",
    "pandas_df_mois = df_mois.toPandas()\n",
    "\n",
    "fig_mois = px.bar(pandas_df_mois, x='mois', y='count', title='Répartition des accidents impliquant des cyclistes/piétons par mois')\n",
    "fig_mois.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage des types composites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fabrication d'un dataframe avec une ligne par accident, une colonne contenant les véhicules en cause, et les lieux de l’accident.\n",
    "\n",
    " Nous allons créer un DataFrame Spark qui rassemble des informations à la fois sur les véhicules impliqués dans les accidents et les lieux où ces accidents se sont produits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggrégation des Informations sur les Véhicules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules_agg = df_vehicules.groupBy(\"Num_Acc\").agg(\n",
    "    fn.collect_list(\n",
    "        fn.struct(\n",
    "            \"id_vehicule\", \"num_veh\", \"senc\", \"catv\", \"obs\", \"obsm\", \"choc\", \"manv\", \"motor\", \"annee\"\n",
    "        )\n",
    "    ).alias(\"vehicules\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faisons une Jointure avec les Informations sur les Lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jointure = df_vehicules_agg.join(df_lieux, \"Num_Acc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et enfin construisons la dataframe composée des informations sur les véhicules et les lieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df_jointure.select(\n",
    "    \"Num_Acc\",\n",
    "    \"vehicules\",\n",
    "    fn.struct(\n",
    "        \"catr\", \"voie\", \"V1\", \"circ\", \"nbv\", \"vosp\", \"prof\", \"pr\", \"pr1\", \"plan\", \"larrout\", \"surf\", \"infra\", \"situ\", \"vma\"\n",
    "    ).alias(\"details_lieu\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde au format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data_cleaned\"\n",
    "os.makedirs(data_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour `caracteristiques`\n",
    "\n",
    "Partitionner par département (`dep`) optimise l'analyse et la gestion des données :\n",
    "\n",
    "- Améliore la performance des requêtes en ciblant des sous-ensembles spécifiques.\n",
    "- Facilite l'organisation et l'accès aux données par région géographique.\n",
    "- Permet des analyses régionales ciblées, aidant à identifier des tendances locales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_caracteristiques.write.partitionBy(\"dep\").parquet(\"data_cleaned/caracteristiques_par_dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour `vehicules`\n",
    "\n",
    "Partitionner la table des véhicules par catégorie (`catv`) présente plusieurs avantages significatifs:\n",
    "\n",
    "- Améliore la performance des requêtes et permet de cibler efficacement des catégories spécifiques de véhicules\n",
    "- Facilite l'organisation des données qui seront structurées de manière intuitive selon le type de véhicule, rendant l'accès et l'analyse plus aisés.\n",
    "- Favorise les études spécifiques par catégorie de véhicule, essentielles pour comprendre les risques et les comportements associés à chaque type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicules.write.partitionBy(\"catv\").parquet(\"data_cleaned/vehicules_par_catv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour `usagers`\n",
    "\n",
    "Partitionner cette table par catégorie d'usager (`catu`) améliore significativement l'analyse et la gestion des données :\n",
    "\n",
    "- Accès rapide aux données concernant des catégories spécifiques d'usagers, comme les conducteurs, les passagers, et les piétons.\n",
    "- Regroupe les données en fonction du rôle de l'usager dans l'accident, facilitant les analyses spécifiques.\n",
    "- Permet des études ciblées sur le comportement et les risques associés à chaque catégorie d'usagers, essentiel pour les initiatives de prévention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usagers.write.partitionBy(\"catu\").parquet(\"data_cleaned/usagers_par_catu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour `lieux`\n",
    "\n",
    "Partitionner cette table par par catégorie de route (`catr`) offre plusieurs avantages pour la gestion et l'analyse des données :\n",
    "\n",
    "- Supporte des études spécifiques aux différents environnements routiers, permettant de détecter les zones à risque et d'adapter les mesures de sécurité.\n",
    "- Aide les autorités à concevoir des politiques et des interventions spécifiques en fonction des caractéristiques de chaque type de route.\n",
    "- La segmentation par type de route aide à mieux organiser les données, rendant les analyses par catégorie de route plus intuitives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux.write.partitionBy(\"catr\").parquet(\"data_cleaned/lieux_par_catr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lieux = spark.read.parquet(\"data_cleaned/lieux_par_catr\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
